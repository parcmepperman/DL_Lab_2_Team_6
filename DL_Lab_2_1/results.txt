Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg
NUM_CHECKPOINTS=5
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos

Loading data...
2018-07-27 14:51:10.724443: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Vocabulary Size: 4135
Train/Dev split: 267/29
Writing to /Users/marcpepperman/Desktop/DeepLearning_ICP_CS490/DL Lab 2 Team 24/DL_Lab_2_1/runs/1532721071

2018-07-27T14:51:15.295100: step 1, loss 2.85876, acc 0.53125
2018-07-27T14:51:19.475712: step 2, loss 2.83185, acc 0.5
2018-07-27T14:51:23.507344: step 3, loss 2.55445, acc 0.484375
2018-07-27T14:51:27.441599: step 4, loss 2.2454, acc 0.515625
2018-07-27T14:51:28.139210: step 5, loss 1.70821, acc 0.636364
2018-07-27T14:51:31.969488: step 6, loss 1.94381, acc 0.5625
2018-07-27T14:51:36.018937: step 7, loss 2.30016, acc 0.515625
2018-07-27T14:51:40.438793: step 8, loss 1.90841, acc 0.625
2018-07-27T14:51:44.210239: step 9, loss 2.30243, acc 0.515625
2018-07-27T14:51:44.880507: step 10, loss 1.83087, acc 0.727273
2018-07-27T14:51:48.639287: step 11, loss 2.87148, acc 0.5625
2018-07-27T14:51:52.302855: step 12, loss 1.2828, acc 0.703125
2018-07-27T14:51:55.958911: step 13, loss 2.3563, acc 0.53125
2018-07-27T14:51:59.659381: step 14, loss 1.33171, acc 0.640625
2018-07-27T14:52:00.296172: step 15, loss 2.80594, acc 0.363636
2018-07-27T14:52:03.887365: step 16, loss 1.34804, acc 0.71875
2018-07-27T14:52:07.470793: step 17, loss 1.98705, acc 0.53125
2018-07-27T14:52:11.053975: step 18, loss 1.72412, acc 0.609375
2018-07-27T14:52:14.626063: step 19, loss 2.25068, acc 0.59375
2018-07-27T14:52:15.271128: step 20, loss 1.21172, acc 0.636364
2018-07-27T14:52:18.932128: step 21, loss 1.633, acc 0.640625
2018-07-27T14:52:22.613480: step 22, loss 0.927739, acc 0.734375
2018-07-27T14:52:26.355405: step 23, loss 1.78337, acc 0.6875
2018-07-27T14:52:30.082160: step 24, loss 1.09738, acc 0.78125
2018-07-27T14:52:30.746705: step 25, loss 1.2674, acc 0.545455
2018-07-27T14:52:34.505768: step 26, loss 1.23537, acc 0.671875
2018-07-27T14:52:38.118748: step 27, loss 1.64608, acc 0.734375
2018-07-27T14:52:41.938301: step 28, loss 1.22471, acc 0.765625
2018-07-27T14:52:45.836058: step 29, loss 1.46273, acc 0.671875
2018-07-27T14:52:46.481273: step 30, loss 2.44162, acc 0.636364
2018-07-27T14:52:50.087708: step 31, loss 1.01268, acc 0.734375
2018-07-27T14:52:53.651465: step 32, loss 0.884711, acc 0.71875
2018-07-27T14:52:57.242346: step 33, loss 1.00081, acc 0.71875
2018-07-27T14:53:00.807120: step 34, loss 1.26608, acc 0.671875
2018-07-27T14:53:01.447405: step 35, loss 1.08298, acc 0.727273
2018-07-27T14:53:05.062695: step 36, loss 0.828924, acc 0.6875
2018-07-27T14:53:08.608976: step 37, loss 1.04676, acc 0.65625
2018-07-27T14:53:12.177516: step 38, loss 0.978612, acc 0.6875
2018-07-27T14:53:15.735008: step 39, loss 0.830009, acc 0.78125
2018-07-27T14:53:16.364982: step 40, loss 1.18218, acc 0.636364
2018-07-27T14:53:19.979023: step 41, loss 0.982779, acc 0.78125
2018-07-27T14:53:24.092034: step 42, loss 0.401968, acc 0.875
2018-07-27T14:53:30.067864: step 43, loss 0.882221, acc 0.828125
2018-07-27T14:53:34.631908: step 44, loss 0.872141, acc 0.765625
2018-07-27T14:53:35.373932: step 45, loss 2.1711, acc 0.636364
2018-07-27T14:53:39.473332: step 46, loss 1.01154, acc 0.8125
2018-07-27T14:53:43.802446: step 47, loss 0.817797, acc 0.796875
2018-07-27T14:53:47.727087: step 48, loss 1.20679, acc 0.671875
2018-07-27T14:53:51.720783: step 49, loss 1.35005, acc 0.703125
2018-07-27T14:53:52.440213: step 50, loss 0.395968, acc 0.818182
2018-07-27T14:53:57.050573: step 51, loss 0.882636, acc 0.765625
2018-07-27T14:54:00.682945: step 52, loss 0.471726, acc 0.890625
2018-07-27T14:54:04.457948: step 53, loss 0.71097, acc 0.8125
2018-07-27T14:54:08.081303: step 54, loss 0.929748, acc 0.78125
2018-07-27T14:54:08.735343: step 55, loss 1.33472, acc 0.636364
2018-07-27T14:54:12.489934: step 56, loss 0.978802, acc 0.796875
2018-07-27T14:54:16.202055: step 57, loss 0.545795, acc 0.84375
2018-07-27T14:54:19.817726: step 58, loss 0.491124, acc 0.8125
2018-07-27T14:54:23.440054: step 59, loss 0.699933, acc 0.765625
2018-07-27T14:54:24.089572: step 60, loss 0.444931, acc 0.818182
2018-07-27T14:54:27.741430: step 61, loss 0.479604, acc 0.765625
2018-07-27T14:54:31.385460: step 62, loss 0.89206, acc 0.75
2018-07-27T14:54:35.158651: step 63, loss 0.523495, acc 0.8125
2018-07-27T14:54:39.280937: step 64, loss 0.765567, acc 0.796875
2018-07-27T14:54:40.060434: step 65, loss 1.21483, acc 0.727273
2018-07-27T14:54:43.908498: step 66, loss 0.904293, acc 0.78125
2018-07-27T14:54:47.838794: step 67, loss 0.442744, acc 0.796875
2018-07-27T14:54:51.780939: step 68, loss 0.57192, acc 0.8125
2018-07-27T14:54:55.465756: step 69, loss 0.631916, acc 0.84375
2018-07-27T14:54:56.137657: step 70, loss 0.773811, acc 0.727273
2018-07-27T14:55:00.136784: step 71, loss 0.64217, acc 0.828125
2018-07-27T14:55:03.982659: step 72, loss 0.247879, acc 0.921875
2018-07-27T14:55:07.880057: step 73, loss 0.864979, acc 0.828125
2018-07-27T14:55:11.821171: step 74, loss 0.784202, acc 0.765625
2018-07-27T14:55:12.475330: step 75, loss 1.26337, acc 0.727273
2018-07-27T14:55:16.213677: step 76, loss 0.616661, acc 0.84375
2018-07-27T14:55:20.206776: step 77, loss 0.530938, acc 0.859375
2018-07-27T14:55:24.463408: step 78, loss 0.462635, acc 0.859375
2018-07-27T14:55:28.582126: step 79, loss 0.327457, acc 0.859375
2018-07-27T14:55:29.270398: step 80, loss 0.380318, acc 0.909091
2018-07-27T14:55:33.330067: step 81, loss 0.312877, acc 0.890625
2018-07-27T14:55:37.044420: step 82, loss 0.72058, acc 0.765625
2018-07-27T14:55:41.030347: step 83, loss 0.328257, acc 0.890625
2018-07-27T14:55:45.057749: step 84, loss 0.221201, acc 0.921875
2018-07-27T14:55:45.722900: step 85, loss 0.0198472, acc 1
2018-07-27T14:55:49.934799: step 86, loss 0.725652, acc 0.828125
2018-07-27T14:55:54.002972: step 87, loss 0.510588, acc 0.859375
2018-07-27T14:55:57.822654: step 88, loss 0.746485, acc 0.828125
2018-07-27T14:56:01.769508: step 89, loss 0.340191, acc 0.859375
2018-07-27T14:56:02.418841: step 90, loss 0.0158997, acc 1
2018-07-27T14:56:06.270959: step 91, loss 0.622006, acc 0.78125
2018-07-27T14:56:10.233902: step 92, loss 0.176054, acc 0.9375
2018-07-27T14:56:14.106279: step 93, loss 0.679683, acc 0.828125
2018-07-27T14:56:18.142061: step 94, loss 0.317161, acc 0.890625
2018-07-27T14:56:18.807621: step 95, loss 0.418783, acc 0.818182
2018-07-27T14:56:23.333664: step 96, loss 0.310052, acc 0.859375
2018-07-27T14:56:27.175670: step 97, loss 0.459173, acc 0.859375
2018-07-27T14:56:31.930702: step 98, loss 0.293021, acc 0.90625
2018-07-27T14:56:36.061040: step 99, loss 0.432146, acc 0.859375
2018-07-27T14:56:36.712784: step 100, loss 0.62856, acc 0.818182

Evaluation:
2018-07-27T14:56:37.216252: step 100, loss 0.970692, acc 0.482759

Saved model checkpoint to /Users/marcpepperman/Desktop/DeepLearning_ICP_CS490/DL Lab 2 Team 24/DL_Lab_2_1/runs/1532721071/checkpoints/model-100

2018-07-27T14:56:41.415886: step 101, loss 0.29964, acc 0.890625
2018-07-27T14:56:46.472005: step 102, loss 0.24453, acc 0.9375
2018-07-27T14:56:50.632254: step 103, loss 0.449851, acc 0.859375
2018-07-27T14:56:54.649619: step 104, loss 0.379853, acc 0.90625
2018-07-27T14:56:55.396196: step 105, loss 0.246427, acc 0.909091
2018-07-27T14:56:59.913051: step 106, loss 0.211329, acc 0.921875
2018-07-27T14:57:04.251035: step 107, loss 0.473873, acc 0.796875
2018-07-27T14:57:08.358065: step 108, loss 0.664834, acc 0.828125
2018-07-27T14:57:12.158957: step 109, loss 0.182577, acc 0.90625
2018-07-27T14:57:13.020205: step 110, loss 0.220146, acc 0.818182
2018-07-27T14:57:17.095709: step 111, loss 0.549043, acc 0.84375
2018-07-27T14:57:20.824068: step 112, loss 0.236161, acc 0.890625
2018-07-27T14:57:24.853412: step 113, loss 0.121744, acc 0.953125
2018-07-27T14:57:28.988185: step 114, loss 0.414709, acc 0.84375
2018-07-27T14:57:29.711540: step 115, loss 0.555394, acc 0.818182
2018-07-27T14:57:34.040789: step 116, loss 0.465082, acc 0.859375
2018-07-27T14:57:38.568264: step 117, loss 0.326587, acc 0.875
2018-07-27T14:57:43.386130: step 118, loss 0.24189, acc 0.9375
2018-07-27T14:57:48.486623: step 119, loss 0.303824, acc 0.921875
2018-07-27T14:57:49.295981: step 120, loss 0.0461215, acc 1
2018-07-27T14:57:53.321672: step 121, loss 0.312927, acc 0.921875
2018-07-27T14:57:57.557839: step 122, loss 0.398146, acc 0.875
2018-07-27T14:58:01.690431: step 123, loss 0.238629, acc 0.953125
2018-07-27T14:58:05.577882: step 124, loss 0.279731, acc 0.890625
2018-07-27T14:58:06.309641: step 125, loss 0.264234, acc 0.818182
2018-07-27T14:58:10.160089: step 126, loss 0.392516, acc 0.84375
2018-07-27T14:58:13.786223: step 127, loss 0.164276, acc 0.953125
2018-07-27T14:58:17.672453: step 128, loss 0.171852, acc 0.921875
2018-07-27T14:58:21.527357: step 129, loss 0.185066, acc 0.921875
2018-07-27T14:58:22.300563: step 130, loss 0.412285, acc 0.818182
2018-07-27T14:58:26.856216: step 131, loss 0.179719, acc 0.921875
2018-07-27T14:58:31.097608: step 132, loss 0.538494, acc 0.859375
2018-07-27T14:58:34.812431: step 133, loss 0.316313, acc 0.890625
2018-07-27T14:58:39.003671: step 134, loss 0.231296, acc 0.890625
2018-07-27T14:58:39.673657: step 135, loss 0.885431, acc 0.818182
2018-07-27T14:58:43.741086: step 136, loss 0.538877, acc 0.84375
2018-07-27T14:58:48.347972: step 137, loss 0.167143, acc 0.9375
2018-07-27T14:58:52.385645: step 138, loss 0.222851, acc 0.875
2018-07-27T14:58:57.093603: step 139, loss 0.460025, acc 0.828125
2018-07-27T14:58:57.985982: step 140, loss 0.196142, acc 0.909091
2018-07-27T14:59:01.625532: step 141, loss 0.22699, acc 0.90625
2018-07-27T14:59:06.125749: step 142, loss 0.0999034, acc 0.9375
2018-07-27T14:59:10.417038: step 143, loss 0.113604, acc 0.953125
2018-07-27T14:59:14.240243: step 144, loss 0.142101, acc 0.9375
2018-07-27T14:59:14.877907: step 145, loss 0.245205, acc 0.909091
2018-07-27T14:59:18.692200: step 146, loss 0.217355, acc 0.953125
2018-07-27T14:59:22.458700: step 147, loss 0.389715, acc 0.890625
2018-07-27T14:59:26.588225: step 148, loss 0.178241, acc 0.921875
2018-07-27T14:59:30.728779: step 149, loss 0.485663, acc 0.875
2018-07-27T14:59:31.511265: step 150, loss 0.0140829, acc 1
2018-07-27T14:59:35.433350: step 151, loss 0.167029, acc 0.9375
2018-07-27T14:59:39.600615: step 152, loss 0.192923, acc 0.9375
2018-07-27T14:59:43.602526: step 153, loss 0.12274, acc 0.96875
2018-07-27T14:59:47.544367: step 154, loss 0.298, acc 0.921875
2018-07-27T14:59:48.228394: step 155, loss 0.504406, acc 0.818182
2018-07-27T14:59:52.346389: step 156, loss 0.243804, acc 0.90625
2018-07-27T14:59:56.442373: step 157, loss 0.185085, acc 0.921875
2018-07-27T15:00:00.288820: step 158, loss 0.288334, acc 0.90625
2018-07-27T15:00:04.405608: step 159, loss 0.461189, acc 0.875
2018-07-27T15:00:05.132529: step 160, loss 0.319964, acc 0.909091
2018-07-27T15:00:09.177549: step 161, loss 0.322354, acc 0.90625
2018-07-27T15:00:13.453211: step 162, loss 0.257047, acc 0.875
2018-07-27T15:00:17.432829: step 163, loss 0.253887, acc 0.9375
2018-07-27T15:00:21.741387: step 164, loss 0.386528, acc 0.859375
2018-07-27T15:00:22.458665: step 165, loss 0.0449116, acc 1
2018-07-27T15:00:27.239852: step 166, loss 0.326514, acc 0.875
2018-07-27T15:00:31.784289: step 167, loss 0.0442698, acc 0.984375
2018-07-27T15:00:36.834917: step 168, loss 0.0656016, acc 0.96875
2018-07-27T15:00:41.039198: step 169, loss 0.143601, acc 0.921875
2018-07-27T15:00:41.995194: step 170, loss 0.0433251, acc 1
2018-07-27T15:00:46.867729: step 171, loss 0.0805053, acc 0.984375
2018-07-27T15:00:51.124068: step 172, loss 0.126243, acc 0.90625
2018-07-27T15:00:55.791144: step 173, loss 0.118168, acc 0.953125
2018-07-27T15:00:59.982617: step 174, loss 0.206385, acc 0.90625
2018-07-27T15:01:00.631560: step 175, loss 0.310534, acc 0.909091
2018-07-27T15:01:04.977383: step 176, loss 0.0789043, acc 0.953125
2018-07-27T15:01:09.214776: step 177, loss 0.215534, acc 0.90625
2018-07-27T15:01:13.059346: step 178, loss 0.167238, acc 0.9375
2018-07-27T15:01:17.303783: step 179, loss 0.03858, acc 0.984375
2018-07-27T15:01:18.062115: step 180, loss 0.00391488, acc 1
2018-07-27T15:01:21.893151: step 181, loss 0.144679, acc 0.9375
2018-07-27T15:01:26.236130: step 182, loss 0.144923, acc 0.9375
2018-07-27T15:01:31.301458: step 183, loss 0.131652, acc 0.90625
2018-07-27T15:01:35.859975: step 184, loss 0.221856, acc 0.921875
2018-07-27T15:01:36.630299: step 185, loss 0.227045, acc 0.818182
2018-07-27T15:01:41.177132: step 186, loss 0.348455, acc 0.890625
2018-07-27T15:01:46.060604: step 187, loss 0.170334, acc 0.921875
2018-07-27T15:01:50.193682: step 188, loss 0.215197, acc 0.859375
2018-07-27T15:01:53.998473: step 189, loss 0.221225, acc 0.90625
2018-07-27T15:01:54.656556: step 190, loss 0.511478, acc 0.818182
2018-07-27T15:01:58.988626: step 191, loss 0.289799, acc 0.890625
2018-07-27T15:02:02.897667: step 192, loss 0.157548, acc 0.9375
2018-07-27T15:02:07.497111: step 193, loss 0.0593526, acc 0.984375
2018-07-27T15:02:11.773890: step 194, loss 0.205532, acc 0.921875
2018-07-27T15:02:12.507894: step 195, loss 0.00155935, acc 1
2018-07-27T15:02:16.887863: step 196, loss 0.0639324, acc 0.984375
2018-07-27T15:02:21.121869: step 197, loss 0.186258, acc 0.921875
2018-07-27T15:02:25.739925: step 198, loss 0.307519, acc 0.90625
2018-07-27T15:02:30.101325: step 199, loss 0.107543, acc 0.96875
2018-07-27T15:02:30.986106: step 200, loss 0.015135, acc 1

Evaluation:
2018-07-27T15:02:31.517528: step 200, loss 0.985719, acc 0.37931

Saved model checkpoint to /Users/marcpepperman/Desktop/DeepLearning_ICP_CS490/DL Lab 2 Team 24/DL_Lab_2_1/runs/1532721071/checkpoints/model-200

2018-07-27T15:02:35.810258: step 201, loss 0.182667, acc 0.953125
2018-07-27T15:02:40.292131: step 202, loss 0.16994, acc 0.9375
2018-07-27T15:02:44.717561: step 203, loss 0.320917, acc 0.9375
2018-07-27T15:02:48.912878: step 204, loss 0.13034, acc 0.96875
2018-07-27T15:02:49.619286: step 205, loss 0.0871802, acc 0.909091
2018-07-27T15:02:53.459141: step 206, loss 0.104113, acc 0.96875
2018-07-27T15:02:57.096819: step 207, loss 0.191854, acc 0.953125
2018-07-27T15:03:00.964716: step 208, loss 0.199247, acc 0.921875
2018-07-27T15:03:04.724477: step 209, loss 0.0948648, acc 0.953125
2018-07-27T15:03:05.395651: step 210, loss 0.0966877, acc 0.909091
2018-07-27T15:03:09.500900: step 211, loss 0.248571, acc 0.90625
2018-07-27T15:03:13.302295: step 212, loss 0.0999913, acc 0.953125
2018-07-27T15:03:17.780812: step 213, loss 0.158212, acc 0.9375
2018-07-27T15:03:22.038925: step 214, loss 0.0770983, acc 0.96875
2018-07-27T15:03:22.914270: step 215, loss 0.150127, acc 0.909091
2018-07-27T15:03:27.132923: step 216, loss 0.271981, acc 0.90625
2018-07-27T15:03:31.483587: step 217, loss 0.27114, acc 0.890625
2018-07-27T15:03:35.431776: step 218, loss 0.0262271, acc 1
2018-07-27T15:03:39.423379: step 219, loss 0.137611, acc 0.9375
2018-07-27T15:03:40.113407: step 220, loss 0.0396156, acc 1
2018-07-27T15:03:44.338420: step 221, loss 0.201497, acc 0.953125
2018-07-27T15:03:49.603984: step 222, loss 0.0413097, acc 0.984375
2018-07-27T15:03:54.744193: step 223, loss 0.062343, acc 0.96875
2018-07-27T15:03:58.711487: step 224, loss 0.0854628, acc 0.96875
2018-07-27T15:03:59.558283: step 225, loss 0.137833, acc 0.909091
2018-07-27T15:04:04.072542: step 226, loss 0.227803, acc 0.9375
2018-07-27T15:04:08.158165: step 227, loss 0.0407432, acc 0.96875
2018-07-27T15:04:11.888342: step 228, loss 0.152251, acc 0.96875
2018-07-27T15:04:15.860464: step 229, loss 0.156844, acc 0.9375
2018-07-27T15:04:16.542210: step 230, loss 0.0389836, acc 1
2018-07-27T15:04:20.645958: step 231, loss 0.0728593, acc 0.984375
2018-07-27T15:04:24.635530: step 232, loss 0.118139, acc 0.953125
2018-07-27T15:04:28.650837: step 233, loss 0.13894, acc 0.953125
2018-07-27T15:04:32.403397: step 234, loss 0.142499, acc 0.953125
2018-07-27T15:04:33.091144: step 235, loss 0.0822912, acc 0.909091
2018-07-27T15:04:37.074569: step 236, loss 0.141813, acc 0.953125
2018-07-27T15:04:41.108642: step 237, loss 0.059074, acc 0.96875
2018-07-27T15:04:45.106635: step 238, loss 0.0583627, acc 0.96875
2018-07-27T15:04:49.365191: step 239, loss 0.150604, acc 0.953125
2018-07-27T15:04:50.050326: step 240, loss 0.0130386, acc 1
2018-07-27T15:04:54.076072: step 241, loss 0.343188, acc 0.921875
2018-07-27T15:04:59.583916: step 242, loss 0.0405328, acc 0.984375
2018-07-27T15:05:04.502806: step 243, loss 0.0452673, acc 0.984375
2018-07-27T15:05:08.985150: step 244, loss 0.0648318, acc 0.984375
2018-07-27T15:05:09.754541: step 245, loss 0.0364584, acc 1
2018-07-27T15:05:13.831791: step 246, loss 0.186819, acc 0.9375
2018-07-27T15:05:17.773808: step 247, loss 0.116395, acc 0.953125
2018-07-27T15:05:21.909897: step 248, loss 0.022497, acc 1
2018-07-27T15:05:26.089708: step 249, loss 0.0917366, acc 0.953125
2018-07-27T15:05:27.049719: step 250, loss 0.0742986, acc 1
2018-07-27T15:05:32.371968: step 251, loss 0.165514, acc 0.96875
2018-07-27T15:05:36.631170: step 252, loss 0.103993, acc 0.953125
2018-07-27T15:05:40.688236: step 253, loss 0.111983, acc 0.96875
2018-07-27T15:05:44.502389: step 254, loss 0.114084, acc 0.953125
2018-07-27T15:05:45.178487: step 255, loss 0.162402, acc 0.909091
2018-07-27T15:05:49.290803: step 256, loss 0.202514, acc 0.9375
2018-07-27T15:05:53.381848: step 257, loss 0.136976, acc 0.921875
2018-07-27T15:05:57.467138: step 258, loss 0.071325, acc 0.9375
2018-07-27T15:06:01.496311: step 259, loss 0.0824696, acc 0.96875
2018-07-27T15:06:02.162464: step 260, loss 0.302378, acc 0.909091
2018-07-27T15:06:06.384029: step 261, loss 0.207774, acc 0.9375
2018-07-27T15:06:10.644533: step 262, loss 0.131764, acc 0.9375
2018-07-27T15:06:15.506491: step 263, loss 0.0785512, acc 0.96875
2018-07-27T15:06:20.443628: step 264, loss 0.105817, acc 0.953125
2018-07-27T15:06:21.469250: step 265, loss 0.0128234, acc 1
2018-07-27T15:06:26.103810: step 266, loss 0.0627651, acc 0.96875
2018-07-27T15:06:30.380206: step 267, loss 0.100659, acc 0.953125
2018-07-27T15:06:34.547844: step 268, loss 0.0720901, acc 0.953125
2018-07-27T15:06:38.804419: step 269, loss 0.17148, acc 0.9375
2018-07-27T15:06:39.592268: step 270, loss 0.0315228, acc 1
2018-07-27T15:06:43.358586: step 271, loss 0.0612529, acc 0.96875
2018-07-27T15:06:47.387861: step 272, loss 0.0370662, acc 1
2018-07-27T15:06:51.499374: step 273, loss 0.0690971, acc 0.984375
2018-07-27T15:06:55.437361: step 274, loss 0.0814582, acc 0.9375
2018-07-27T15:06:56.261878: step 275, loss 0.0010177, acc 1
2018-07-27T15:07:00.602143: step 276, loss 0.131066, acc 0.953125
2018-07-27T15:07:04.421408: step 277, loss 0.28783, acc 0.921875
2018-07-27T15:07:08.517784: step 278, loss 0.111257, acc 0.953125
2018-07-27T15:07:12.767218: step 279, loss 0.0534609, acc 0.96875
2018-07-27T15:07:13.450187: step 280, loss 0.00116843, acc 1
2018-07-27T15:07:17.311720: step 281, loss 0.207386, acc 0.9375
2018-07-27T15:07:20.966460: step 282, loss 0.130136, acc 0.953125
2018-07-27T15:07:25.010224: step 283, loss 0.0993548, acc 0.96875
2018-07-27T15:07:28.809447: step 284, loss 0.116023, acc 0.953125
2018-07-27T15:07:29.458558: step 285, loss 0.0137153, acc 1
2018-07-27T15:07:33.365201: step 286, loss 0.0794414, acc 0.953125
2018-07-27T15:07:37.271951: step 287, loss 0.0555276, acc 0.96875
2018-07-27T15:07:40.897433: step 288, loss 0.172194, acc 0.953125
2018-07-27T15:07:44.903665: step 289, loss 0.123194, acc 0.953125
2018-07-27T15:07:45.577035: step 290, loss 0.207059, acc 0.909091
2018-07-27T15:07:49.772290: step 291, loss 0.116511, acc 0.953125
2018-07-27T15:07:53.865606: step 292, loss 0.0913584, acc 0.984375
2018-07-27T15:07:58.418423: step 293, loss 0.0193218, acc 0.984375
2018-07-27T15:08:02.894145: step 294, loss 0.170341, acc 0.9375
2018-07-27T15:08:03.762252: step 295, loss 0.00174676, acc 1
2018-07-27T15:08:08.378319: step 296, loss 0.105717, acc 0.984375
2018-07-27T15:08:12.398258: step 297, loss 0.0797236, acc 0.984375
2018-07-27T15:08:16.317144: step 298, loss 0.158658, acc 0.9375
2018-07-27T15:08:20.210522: step 299, loss 0.268913, acc 0.9375
2018-07-27T15:08:21.067418: step 300, loss 0.0799903, acc 1

Evaluation:
2018-07-27T15:08:21.763607: step 300, loss 0.918991, acc 0.413793
